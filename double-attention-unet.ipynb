{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9351402,"sourceType":"datasetVersion","datasetId":5668572},{"sourceId":9894457,"sourceType":"datasetVersion","datasetId":6077203},{"sourceId":9941027,"sourceType":"datasetVersion","datasetId":5174487},{"sourceId":10731327,"sourceType":"datasetVersion","datasetId":6653355}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Importing all necessary libraries","metadata":{}},{"cell_type":"code","source":"\n# Basic data manipulations\nimport pandas as pd\nimport numpy as np\n\n# Handling images\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Handling paths\n\nimport time\n\n# Pytorch essentials\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision.datasets import ImageFolder\n\n\n# Pytorch essentials for datasets.\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Pytorch way of data augmentation.\nimport torchvision\nfrom torchvision import datasets, models, transforms, utils\nfrom torchvision.transforms import v2\n\nimport cv2\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport shutil\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import confusion_matrix , accuracy_score, classification_report\nimport seaborn as sns\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n! pip install segmentation-models-pytorch\nimport segmentation_models_pytorch as smp\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ISIC 2016 Dataloader","metadata":{}},{"cell_type":"code","source":"\ntrain_img_df = pd.read_csv('/kaggle/input/isic-2016-dataset/train_ISIC_2016.csv')\ntrain_img_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nval_img_df = pd.read_csv('/kaggle/input/isic-2016-dataset/val_ISIC_2016.csv')\nval_img_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_img_df = pd.read_csv('/kaggle/input/isic-2016-dataset/test_ISIC_2016.csv')\ntest_img_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_img = train_img_df['Image_Id'].tolist()\ntrain_img_paths = []\ntrain_mask_paths = []\nfor i in range(len(train_img)):\n    train_img_paths.append(\"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Training_Data/\" + train_img[i])\n    train_mask_paths.append(\"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Training_GroundTruth/\" + train_img[i][:-4] + \"_Segmentation\" + \".png\")\nprint(len(train_img_paths))\nprint(len(train_mask_paths))\n\ntrain_df = pd.DataFrame({\"images\":train_img_paths,\"masks\":train_mask_paths})\ntrain_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nval_img = val_img_df['Image_Id'].tolist()\nval_img_paths = []\nval_mask_paths = []\nfor i in range(len(val_img)):\n    val_img_paths.append(\"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Training_Data/\" + val_img[i])\n    val_mask_paths.append(\"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Training_GroundTruth/\" + val_img[i][:-4] + \"_Segmentation\" + \".png\")\nprint(len(val_img_paths))\nprint(len(val_mask_paths))\n\nval_df = pd.DataFrame({\"images\":val_img_paths,\"masks\":val_mask_paths})\nval_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_img = test_img_df['Image_Id'].tolist()\ntest_img_paths = []\ntest_mask_paths = []\nfor i in range(len(test_img)):\n    test_img_paths.append(\"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Test_Data/\" + test_img[i])\n    test_mask_paths.append(\"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Test_GroundTruth/\" + test_img[i][:-4] + \"_Segmentation\" + \".png\")\nprint(len(test_img_paths))\nprint(len(test_mask_paths))\n\ntest_df = pd.DataFrame({\"images\":test_img_paths,\"masks\":test_mask_paths})\ntest_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CPM-17 Dataloader","metadata":{}},{"cell_type":"code","source":"# read img and mask\ntrain_img_paths = sorted(glob('/kaggle/working/Cervical_Segmentation_dataset/train/images/*.bmp'))\ntrain_mask_paths = sorted(glob('/kaggle/working/Cervical_Segmentation_dataset/train/masks/*.png'))\ntrain_df = pd.DataFrame({\"images\":train_img_paths,\"masks\":train_mask_paths})\ntrain_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read img and mask\nvalid_img_paths = sorted(glob('/kaggle/working/Cervical_Segmentation_dataset/val/images/*.bmp'))\nvalid_mask_paths = sorted(glob('/kaggle/working/Cervical_Segmentation_dataset/val/masks/*.png'))\nval_df = pd.DataFrame({\"images\":valid_img_paths,\"masks\":valid_mask_paths})\nval_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read img and mask\ntest_img_paths = sorted(glob('/kaggle/working/Cervical_Segmentation_dataset/test/images/*.bmp'))\ntest_mask_paths = sorted(glob('/kaggle/working/Cervical_Segmentation_dataset/test/masks/*.png'))\ntest_df = pd.DataFrame({\"images\":test_img_paths,\"masks\":test_mask_paths})\ntest_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"MonuSeg Dataloader","metadata":{}},{"cell_type":"code","source":"\n# read img and mask\ntrain_img_paths = sorted(glob('/kaggle/input/monuseg/MonuSeg/Training/Images/*.png'))\ntrain_mask_paths = sorted(glob('/kaggle/input/monuseg/MonuSeg/Training/Masks/*.png'))\ntrain_df = pd.DataFrame({\"images\":train_img_paths,\"masks\":train_mask_paths})\ntrain_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# read img and mask\nval_img_paths = sorted(glob('/kaggle/input/monuseg/MonuSeg/Val/Images/*.png'))\nval_mask_paths = sorted(glob('/kaggle/input/monuseg/MonuSeg/Val/Masks/*.png'))\nval_df = pd.DataFrame({\"images\":val_img_paths,\"masks\":val_mask_paths})\nval_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# read img and mask\ntest_img_paths = sorted(glob('/kaggle/input/monuseg/MonuSeg/Test/Images/*.png'))\ntest_mask_paths = sorted(glob('/kaggle/input/monuseg/MonuSeg/Test/Masks/*.png'))\ntest_df = pd.DataFrame({\"images\":test_img_paths,\"masks\":test_mask_paths})\ntest_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cervical_Cancer_Segmentation_Dataset Loader","metadata":{}},{"cell_type":"code","source":"\n# read img and mask\ntrain_img_paths = sorted(glob('/kaggle/input/cervical-cancer-segmentation/CervicalCancer/train/images/*.bmp'))\ntrain_mask_paths = sorted(glob('/kaggle/input/cervical-cancer-segmentation/CervicalCancer/train/masks/*.bmp'))\ntrain_df = pd.DataFrame({\"images\":train_img_paths,\"masks\":train_mask_paths})\ntrain_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# read img and mask\nval_img_paths = sorted(glob('/kaggle/input/cervical-cancer-segmentation/CervicalCancer/val/images/*.bmp'))\nval_mask_paths = sorted(glob('/kaggle/input/cervical-cancer-segmentation/CervicalCancer/val/masks/*.bmp'))\nval_df = pd.DataFrame({\"images\":val_img_paths,\"masks\":val_mask_paths})\nval_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# read img and mask\ntest_img_paths = sorted(glob('/kaggle/input/cervical-cancer-segmentation/CervicalCancer/test/images/*.bmp'))\ntest_mask_paths = sorted(glob('/kaggle/input/cervical-cancer-segmentation/CervicalCancer/test/masks/*.bmp'))\ntest_df = pd.DataFrame({\"images\":test_img_paths,\"masks\":test_mask_paths})\ntest_df.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Showing some images and their corresponding Ground Truth Masks","metadata":{}},{"cell_type":"code","source":"\nshow_imgs = 4\nidx = np.random.choice(len(train_df), show_imgs, replace=False)\nfig, axes = plt.subplots(show_imgs*2//4, 4, figsize=(15, 8))\naxes = axes.flatten()\nfor i, ax in enumerate(axes):\n    new_i = i//2\n    if i % 2 ==0 :\n        full_path = train_df.loc[idx[new_i]]['images']\n        basename = os.path.basename(full_path) \n    else:\n        full_path = train_df.loc[idx[new_i]]['masks']\n        basename = os.path.basename(full_path) + ' -mask' \n    ax.imshow(plt.imread(full_path))\n    ax.set_title(basename)\n    ax.set_axis_off()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Image resizing and dataset Loading","metadata":{}},{"cell_type":"code","source":"\ntrain_transforms = A.Compose([\n    A.Resize(512, 512),\n    A.RandomCrop(height=512, width=512, always_apply=True),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=(-0.04,0.04), rotate_limit=(-15,15), p=0.5),\n    # A.Normalize(p=1.0),\n    # ToTensorV2(),\n])\n\ntest_transforms = A.Compose([\n    A.Resize(512, 512),\n    # ToTensorV2(),\n])\n\n\nimport torch\nimport cv2\nimport numpy as np\nimport albumentations as A\n\nclass MyDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, transforms_=None):\n        self.df = dataframe\n        # We'll use transforms for data augmentation and converting PIL images to torch tensors.\n        self.transforms_ = transforms_\n        self.pre_normalize = v2.Compose([\n            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        self.resize = [512, 512]\n        self.class_size = 2\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        img = cv2.cvtColor(cv2.imread(self.df.iloc[index]['images']), cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.df.iloc[index]['masks'],cv2.IMREAD_GRAYSCALE)\n        mask = np.where(mask<127, 0, 1).astype(np.int16)\n        aug = self.transforms_(image=img, mask=mask)\n        img, mask = aug['image'], aug['mask']\n        img = img/255\n        # img = self.pre_normalize(img)\n        img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1)\n        #target = torch.tensor(mask, dtype=torch.long)\n        # Convert target (mask) to tensor and resize it first\n        target = torch.tensor(mask, dtype=torch.float)  # Convert to tensor, shape: [1, 512, 512]\n\n        # Resize the target tensor before creating the sample dictionary\n        #target_resized = target.view(3, self.resize[0], self.resize[1])\n      \n\n        # Now create the sample dictionary with the resized target\n        sample = {'x': img, 'y': target}\n\n        #sample = {'x': img, 'y': target}\n        return sample\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training Dataloader and Validation Dataloader Calling","metadata":{}},{"cell_type":"code","source":"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\ntrain_dataset = MyDataset(train_df, train_transforms)\nval_dataset = MyDataset(val_df, test_transforms)\n\nBATCH_SIZE = 4\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\nprint(f'len train: {len(train_df)}')\nprint(f'len val: {len(val_df)}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of Customized Convolutional and ReLU layers and Attention Modules","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\n\ntry:\n    from inplace_abn import InPlaceABN\nexcept ImportError:\n    InPlaceABN = None\n\n\nclass Conv2dReLU(nn.Sequential):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding=0,\n        stride=1,\n        use_batchnorm=True,\n    ):\n        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n            raise RuntimeError(\n                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n                + \"To install see: https://github.com/mapillary/inplace_abn\"\n            )\n\n        conv = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=not (use_batchnorm),\n        )\n        relu = nn.ReLU(inplace=True)\n\n        if use_batchnorm == \"inplace\":\n            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n            relu = nn.Identity()\n\n        elif use_batchnorm and use_batchnorm != \"inplace\":\n            bn = nn.BatchNorm2d(out_channels)\n\n        else:\n            bn = nn.Identity()\n\n        super(Conv2dReLU, self).__init__(conv, bn, relu)\n\nclass ECA(nn.Module):\n    def __init__(self, in_channels, kernel_size=3):\n        super(ECA, self).__init__()\n        self.in_channels = in_channels\n        self.kernel_size = kernel_size\n\n        # Ensure kernel size is odd\n        if self.kernel_size % 2 == 0:\n            self.kernel_size += 1\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(in_channels, in_channels, kernel_size=self.kernel_size, padding=self.kernel_size // 2, groups=in_channels, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        batch_size, channels, height, width = x.size()\n\n        # Squeeze operation\n        y = self.avg_pool(x).view(batch_size, channels)\n\n        # Channel-wise convolution\n        y = y.unsqueeze(2)  # Add a dimension for Conv1d\n        y = self.conv(y)\n        y = y.view(batch_size, channels, 1)\n\n        # Apply sigmoid activation\n        y = self.sigmoid(y)\n\n        # Scale the input\n        return x * y.view(batch_size, channels, 1, 1)\n\nclass BitPlaneAttention(nn.Module):\n    def __init__(self, in_channels, bit_planes=8):\n        super(BitPlaneAttention, self).__init__()\n\n        self.in_channels = in_channels\n        self.bit_planes = bit_planes\n\n        # A 1x1 convolution to adjust the attention across bit planes\n        self.attention_conv = nn.Conv2d(in_channels * bit_planes, in_channels, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def bit_plane_decomposition(self, x):\n        # Decompose the feature map into binary bit planes\n        bit_planes = []\n        for i in range(self.bit_planes):\n            bit_mask = 1 << i\n            bit_plane = (x.int() & bit_mask) // bit_mask\n            bit_planes.append(bit_plane.float())\n        return torch.stack(bit_planes, dim=1)  # (B, bit_planes, C, H, W)\n\n    def forward(self, x):\n        batch_size, C, H, W = x.size()\n\n        # Decompose into bit planes\n        bit_planes = self.bit_plane_decomposition(x)  # (B, bit_planes, C, H, W)\n        bit_planes = bit_planes.view(batch_size, -1, H, W)  # (B, bit_planes * C, H, W)\n\n        # Apply attention across bit planes\n        attention_map = self.attention_conv(bit_planes)  # (B, C, H, W)\n        attention_map = self.sigmoid(attention_map)  # Apply sigmoid to get values in [0, 1]\n\n        # Multiply attention map with original input\n        out = x * attention_map\n\n        return out\n\nclass ArgMax(nn.Module):\n    def __init__(self, dim=None):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return torch.argmax(x, dim=self.dim)\n\n\nclass Clamp(nn.Module):\n    def __init__(self, min=0, max=1):\n        super().__init__()\n        self.min, self.max = min, max\n\n    def forward(self, x):\n        return torch.clamp(x, self.min, self.max)\n\n\nclass Activation(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n\n        if name is None or name == \"identity\":\n            self.activation = nn.Identity(**params)\n        elif name == \"sigmoid\":\n            self.activation = nn.Sigmoid()\n        elif name == \"softmax2d\":\n            self.activation = nn.Softmax(dim=1, **params)\n        elif name == \"softmax\":\n            self.activation = nn.Softmax(**params)\n        elif name == \"logsoftmax\":\n            self.activation = nn.LogSoftmax(**params)\n        elif name == \"tanh\":\n            self.activation = nn.Tanh()\n        elif name == \"argmax\":\n            self.activation = ArgMax(**params)\n        elif name == \"argmax2d\":\n            self.activation = ArgMax(dim=1, **params)\n        elif name == \"clamp\":\n            self.activation = Clamp(**params)\n        elif callable(name):\n            self.activation = name(**params)\n        else:\n            raise ValueError(\n                f\"Activation should be callable/sigmoid/softmax/logsoftmax/tanh/\"\n                f\"argmax/argmax2d/clamp/None; got {name}\"\n            )\n\n    def forward(self, x):\n        return self.activation(x)\n\n\nclass Attention(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \"eca\":\n            self.attention = ECA(**params)\n        elif name == \"bp\":\n            self.attention = BitPlaneAttention(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of Segmentation Head","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n#####from .modules import Activation\n\n\nclass SegmentationHead(nn.Sequential):\n    def __init__(\n        self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1\n    ):\n        conv2d = nn.Conv2d(\n            in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2\n        )\n        upsampling = (\n            nn.UpsamplingBilinear2d(scale_factor=upsampling)\n            if upsampling > 1\n            else nn.Identity()\n        )\n        activation = Activation(activation)\n        super().__init__(conv2d, upsampling, activation)\n\n\nclass ClassificationHead(nn.Sequential):\n    def __init__(\n        self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None\n    ):\n        if pooling not in (\"max\", \"avg\"):\n            raise ValueError(\n                \"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling)\n            )\n        pool = nn.AdaptiveAvgPool2d(1) if pooling == \"avg\" else nn.AdaptiveMaxPool2d(1)\n        flatten = nn.Flatten()\n        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n        linear = nn.Linear(in_channels, classes, bias=True)\n        activation = Activation(activation)\n        super().__init__(pool, flatten, dropout, linear, activation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of Segmentation Model Abstract Architecture: Encoder, Decoder and Segmentation Head","metadata":{}},{"cell_type":"code","source":"\nimport torch\nfrom segmentation_models_pytorch.base import initialization as init\n#####from . import initialization as init\n#####from .hub_mixin import SMPHubMixin\nfrom segmentation_models_pytorch.base.hub_mixin import SMPHubMixin\n\nclass SegmentationModel(torch.nn.Module, SMPHubMixin):\n    def initialize(self):\n        init.initialize_decoder(self.decoder)\n        init.initialize_head(self.segmentation_head)\n        if self.classification_head is not None:\n            init.initialize_head(self.classification_head)\n\n    def check_input_shape(self, x):\n        h, w = x.shape[-2:]\n        output_stride = self.encoder.output_stride\n        if h % output_stride != 0 or w % output_stride != 0:\n            new_h = (\n                (h // output_stride + 1) * output_stride\n                if h % output_stride != 0\n                else h\n            )\n            new_w = (\n                (w // output_stride + 1) * output_stride\n                if w % output_stride != 0\n                else w\n            )\n            raise RuntimeError(\n                f\"Wrong input shape height={h}, width={w}. Expected image height and width \"\n                f\"divisible by {output_stride}. Consider pad your images to shape ({new_h}, {new_w}).\"\n            )\n\n    def forward(self, x):\n        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n\n        self.check_input_shape(x)\n\n        features = self.encoder(x)\n        decoder_output = self.decoder(*features)\n\n        masks = self.segmentation_head(decoder_output)\n\n        if self.classification_head is not None:\n            labels = self.classification_head(features[-1])\n            return masks, labels\n\n        return masks\n\n    @torch.no_grad()\n    def predict(self, x):\n        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)` with `torch.no_grad()`\n\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n\n        Return:\n            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n\n        \"\"\"\n        if self.training:\n            self.eval()\n\n        x = self.forward(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of Decoder Block and Center Block","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n#####from segmentation_models_pytorch.base import modules as md\n\nclass DecoderBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        use_batchnorm=True,\n        attention_type=None,\n    ):\n        super().__init__()\n        self.conv1 = Conv2dReLU(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention1 = Attention(\n            attention_type, in_channels=in_channels + skip_channels\n        )\n        self.conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        self.attention2 = Attention(attention_type, in_channels=out_channels)\n\n    def forward(self, x, skip=None):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        if skip is not None:\n        #    print(\"X=, Skip= \", x.shape, skip.shape)\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(nn.Sequential):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        conv1 = Conv2dReLU(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        conv2 = Conv2dReLU(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            padding=1,\n            use_batchnorm=use_batchnorm,\n        )\n        super().__init__(conv1, conv2)\n\n\nclass UnetDecoder(nn.Module):\n    def __init__(\n        self,\n        encoder_channels,\n        decoder_channels,\n        n_blocks=5,\n        use_batchnorm=True,\n        attention_type=None,\n        center=False,\n    ):\n        super().__init__()\n\n        if n_blocks != len(decoder_channels):\n            raise ValueError(\n                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n                    n_blocks, len(decoder_channels)\n                )\n            )\n\n        # remove first skip with same spatial resolution\n        encoder_channels = encoder_channels[1:]\n        # reverse channels to start from head of encoder\n        encoder_channels = encoder_channels[::-1]\n\n        # computing blocks input and output channels\n        head_channels = encoder_channels[0]\n        in_channels = [head_channels] + list(decoder_channels[:-1])\n        skip_channels = list(encoder_channels[1:]) + [0]\n        out_channels = decoder_channels\n\n        if center:\n            self.center = CenterBlock(\n                head_channels, head_channels, use_batchnorm=use_batchnorm\n            )\n        else:\n            self.center = nn.Identity()\n\n        # combine decoder keyword arguments\n        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n        blocks = [\n            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n        ]\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, *features):\n        features = features[1:]  # remove first skip with same spatial resolution\n        features = features[::-1]  # reverse channels to start from head of encoder\n\n        head = features[0]\n        skips = features[1:]\n\n        x = self.center(head)\n        for i, decoder_block in enumerate(self.blocks):\n            skip = skips[i] if i < len(skips) else None\n            x = decoder_block(x, skip)\n\n        return x\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Installing Pytorch Wavelets for using Wavelet Transforms","metadata":{}},{"cell_type":"code","source":"!rm -rf pytorch_wavelets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/fbcotter/pytorch_wavelets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of Wavelet Transform (HAAR)","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_wavelets import DWTForward, DWTInverse\ndef Wavelet_Transform(image):\n    # Ensure image and wavelet transform are on the same device\n    device = image.device\n    xfm = DWTForward(J=1, wave='haar', mode='zero').to(device)\n\n    # Perform wavelet decomposition\n    Yl, Yh = xfm(image)\n\n    # Upsample Yl to match the input shape\n    Yl_upsampled = F.interpolate(Yl, size=(image.shape[2], image.shape[3]), mode='bilinear', align_corners=False)\n    return Yl_upsampled\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of Encoder (MobileNetV2) with the fusion of ECA and Bit-Plane (BA) attention modules, named, FA.","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport functools\nimport torch.utils.model_zoo as model_zoo\nfrom pretrainedmodels.models.inceptionv4 import InceptionV4, pretrained_settings\n\n\nclass EncoderMixin:\n    \"\"\"Mixin class to add encoder functionality such as:\n    - output channels specification for feature tensors\n    - patching first convolution for arbitrary input channels\n    \"\"\"\n    _output_stride = 32\n\n    @property\n    def out_channels(self):\n        return self._out_channels[:self._depth + 1]\n\n    @property\n    def output_stride(self):\n        return min(self._output_stride, 2 ** self._depth)\n\n    def set_in_channels(self, in_channels, pretrained=True):\n        if in_channels == 3:\n            return\n        self._in_channels = in_channels\n        if self._out_channels[0] == 3:\n            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n        patch_first_conv(self, new_in_channels=in_channels, pretrained=pretrained)\n\n    def make_dilated(self, output_stride):\n        if output_stride == 16:\n            stage_list, dilation_list = [5], [2]\n        elif output_stride == 8:\n            stage_list, dilation_list = [4, 5], [2, 4]\n        else:\n            raise ValueError(\"Output stride must be 16 or 8.\")\n        self._output_stride = output_stride\n        stages = self.get_stages()\n        for stage_idx, dilation_rate in zip(stage_list, dilation_list):\n            replace_strides_with_dilation(stages[stage_idx], dilation_rate)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nclass BitPlaneAttention(nn.Module):\n    \"\"\"Bit Plane Attention Block.\"\"\"\n    def __init__(self, in_channels):\n        super(BitPlaneAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv2d(in_channels // 4, in_channels, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv1(y)\n        y = self.conv2(y)\n        y = self.sigmoid(y)\n        return x * y.expand_as(x)\n\nclass ECA(nn.Module):\n    \"\"\"Efficient Channel Attention Block.\"\"\"\n    def __init__(self, in_channels, gamma=2, b=1):\n        super(ECA, self).__init__()\n        self.gamma = gamma\n        self.b = b\n        kernel_size = int(abs((in_channels / self.gamma) + self.b))\n        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        y = torch.mean(x, dim=(2, 3), keepdim=True)\n        y = y.squeeze(-1).permute(0, 2, 1)  # Convert to (N, C, 1) -> (N, 1, C)\n        y = self.conv(y)\n        y = y.permute(0, 2, 1).unsqueeze(-1)  # Convert back to (N, C, 1, 1)\n        y = self.sigmoid(y)\n        return x * y.expand_as(x)\n\nclass FusedAttention(nn.Module):\n    \"\"\"Fused Attention Block combining BitPlane and ECA Attention.\"\"\"\n    def __init__(self, in_channels):\n        super(FusedAttention, self).__init__()\n        self.bitplane_attention = BitPlaneAttention(in_channels)\n        self.eca_attention = ECA(in_channels)\n\n    def forward(self, x):\n        bitplane_attention = self.bitplane_attention(x)\n        eca_attention = self.eca_attention(x)\n        # Combine them with element-wise averaging\n        return (bitplane_attention  + eca_attention)/2\n\nclass MobileNetV2Encoder(torchvision.models.MobileNetV2, EncoderMixin):\n    def __init__(self, out_channels, depth=5, **kwargs):\n        super().__init__(**kwargs)\n        self._depth = depth\n        self._out_channels = out_channels\n        self._in_channels = 3\n        self.attention_blocks = nn.ModuleList([FusedAttention(c) for c in self._out_channels[1:]])\n        \n        del self.classifier\n\n    def get_stages(self):\n        return [\n            nn.Identity(),\n            self.features[:2],\n            self.features[2:4],\n            self.features[4:7],\n            self.features[7:14],\n            self.features[14:],\n        ]\n\n    def forward(self, x):\n        features = []\n        for i in range(self._depth + 1):\n            x = self.get_stages()[i](x)\n\n            if i == 0:  # Apply Wavelet Transform and Fused Attention only at the first stage\n                wavelet_texture = Wavelet_Transform(x)  # Apply wavelet transform\n                x = torch.mul(x, wavelet_texture) \n                \n            if i > 0:  # Add Fused Attention block only after the first stage\n                x = self.attention_blocks[i - 1](x)\n            features.append(x)\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop(\"last_linear.bias\", None)\n        state_dict.pop(\"last_linear.weight\", None)\n        super().load_state_dict(state_dict, **kwargs)\n\n# Utility Functions\ndef patch_first_conv(model, new_in_channels, default_in_channels=3, pretrained=True):\n    for module in model.modules():\n        if isinstance(module, nn.Conv2d) and module.in_channels == default_in_channels:\n            break\n    weight = module.weight.detach()\n    module.in_channels = new_in_channels\n    if not pretrained:\n        module.weight = nn.parameter.Parameter(\n            torch.Tensor(module.out_channels, new_in_channels // module.groups, *module.kernel_size))\n        module.reset_parameters()\n    elif new_in_channels == 1:\n        module.weight = nn.parameter.Parameter(weight.sum(1, keepdim=True))\n    else:\n        new_weight = torch.Tensor(module.out_channels, new_in_channels // module.groups, *module.kernel_size)\n        for i in range(new_in_channels):\n            new_weight[:, i] = weight[:, i % default_in_channels]\n        new_weight *= (default_in_channels / new_in_channels)\n        module.weight = nn.parameter.Parameter(new_weight)\n\n\ndef replace_strides_with_dilation(module, dilation_rate):\n    for mod in module.modules():\n        if isinstance(mod, nn.Conv2d):\n            mod.stride = (1, 1)\n            mod.dilation = (dilation_rate, dilation_rate)\n            kh, kw = mod.kernel_size\n            mod.padding = ((kh // 2) * dilation_rate, (kw // 2) * dilation_rate)\n            if hasattr(mod, \"static_padding\"):\n                mod.static_padding = nn.Identity()\n\n\n# Preprocessing\ndef preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs):\n    if input_space == \"BGR\":\n        x = x[..., ::-1].copy()\n    if input_range is not None and x.max() > 1 and input_range[1] == 1:\n        x = x / 255.0\n    if mean is not None:\n        x -= np.array(mean)\n    if std is not None:\n        x /= np.array(std)\n    return x\n\n\n# Encoder retrieval functions\nencoders = {\n    \"mobilenet_v2\": {\n        \"encoder\": MobileNetV2Encoder,\n        \"pretrained_settings\": {\n            \"imagenet\": {\n                \"mean\": [0.485, 0.456, 0.406],\n                \"std\": [0.229, 0.224, 0.225],\n                \"url\": \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\",\n                \"input_space\": \"RGB\",\n                \"input_range\": [0, 1],\n            }\n        },\n        \"params\": {\"out_channels\": (3, 16, 24, 32, 96, 1280)},\n    }\n}\n\n\ndef get_encoder(name, in_channels=3, depth=5, weights=None, output_stride=32, **kwargs):\n    if name not in encoders:\n        raise KeyError(f\"Unsupported encoder `{name}`, supported: {list(encoders.keys())}\")\n\n    Encoder = encoders[name][\"encoder\"]\n    params = encoders[name][\"params\"]\n    params.update(depth=depth)\n    encoder = Encoder(**params)\n\n    if weights:\n        settings = encoders[name][\"pretrained_settings\"][weights]\n        # Load only base network weights, SE blocks will be randomly initialized\n        pretrained_dict = model_zoo.load_url(settings[\"url\"])\n        model_dict = encoder.state_dict()\n\n        # Filter out SE block weights, which are not present in pre-trained weights\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and \"se_blocks\" not in k}\n        model_dict.update(pretrained_dict)\n\n        encoder.load_state_dict(model_dict)\n\n    encoder.set_in_channels(in_channels, pretrained=weights is not None)\n    if output_stride != 32:\n        encoder.make_dilated(output_stride)\n\n    return encoder\n\n\n\ndef get_preprocessing_params(encoder_name, pretrained=\"imagenet\"):\n    settings = encoders[encoder_name][\"pretrained_settings\"].get(pretrained)\n    return {\n        \"input_space\": settings.get(\"input_space\", \"RGB\"),\n        \"input_range\": list(settings.get(\"input_range\", [0, 1])),\n        \"mean\": list(settings[\"mean\"]),\n        \"std\": list(settings[\"std\"]),\n    }\n\n\ndef get_preprocessing_fn(encoder_name, pretrained=\"imagenet\"):\n    params = get_preprocessing_params(encoder_name, pretrained)\n    return functools.partial(preprocess_input, **params)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of UNet structure","metadata":{}},{"cell_type":"code","source":"\nfrom typing import Optional, Union, List\n\"\"\"\nfrom segmentation_models_pytorch.encoders import get_encoder\nfrom segmentation_models_pytorch.base import (\n    SegmentationModel,\n    SegmentationHead,\n    ClassificationHead,\n)\n\"\"\"\n#####from .decoder import UnetDecoder\n\nclass Unet(SegmentationModel):\n    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder*\n    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*\n    for fusing decoder blocks with skip connections.\n\n    Args:\n        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n            to extract features of different spatial resolution\n        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n            Default is 5\n        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n            other pretrained weights (see table with available weights for each encoder_name)\n        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n            Length of the list should be the same as **encoder_depth**\n        decoder_use_batchnorm: If **True**, BatchNorm2d layer between Conv2D and Activation layers\n            is used. If **\"inplace\"** InplaceABN will be used, allows to decrease memory consumption.\n            Available options are **True, False, \"inplace\"**\n        decoder_attention_type: Attention module used in decoder of the model. Available options are\n            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n        in_channels: A number of input channels for the model, default is 3 (RGB images)\n        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n        activation: An activation function to apply after the final convolution layer.\n            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n                **callable** and **None**.\n            Default is **None**\n        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n            on top of encoder if **aux_params** is not **None** (default). Supported params:\n                - classes (int): A number of classes\n                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n                - dropout (float): Dropout factor in [0, 1)\n                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n                    (could be **None** to return logits)\n\n    Returns:\n        ``torch.nn.Module``: Unet\n\n    .. _Unet:\n        https://arxiv.org/abs/1505.04597\n\n   \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet34\",\n        encoder_depth: int = 5,\n        encoder_weights: Optional[str] = \"imagenet\",\n        decoder_use_batchnorm: bool = True,\n        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n        decoder_attention_type: Optional[str] = None,\n        in_channels: int = 3,\n        classes: int = 1,\n        activation: Optional[Union[str, callable]] = None,\n        aux_params: Optional[dict] = None,\n    ):\n        super().__init__()\n\n        self.encoder = get_encoder(\n            encoder_name,\n            in_channels=in_channels,\n            depth=encoder_depth,\n            weights=encoder_weights,\n        )\n\n        self.decoder = UnetDecoder(\n            encoder_channels=self.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=encoder_depth,\n            use_batchnorm=decoder_use_batchnorm,\n            center=True if encoder_name.startswith(\"vgg\") else False,\n            attention_type=decoder_attention_type,\n        )\n\n        self.segmentation_head = SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=classes,\n            activation=activation,\n            kernel_size=3,\n        )\n\n        if aux_params is not None:\n            self.classification_head = ClassificationHead(\n                in_channels=self.encoder.out_channels[-1], **aux_params\n            )\n        else:\n            self.classification_head = None\n\n        self.name = \"u-{}\".format(encoder_name)\n        self.initialize()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchinfo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ssl\nssl._create_default_https_context = ssl.create_default_context\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Displaying the architecture of our proposed model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchinfo import summary\n#from model import Unet  # Assuming your model is saved in model.py\n\n# Define the device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass_size = 1\nmodel = Unet(\n    encoder_name=\"mobilenet_v2\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=\"imagenet\",\n    #decoder_attention_type=\"Defattn\",             # use `imagenet` pre-trained weights for encoder initialization\n    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n    classes=class_size                      # model output channels (number of classes in your dataset)\n    #activation=\"softmax\"\n).to(device)\n\n# Get the summary\ninput_size = (1, 3, 512, 512)  # Batch size of 1, 3 channels, 512x512 image\nsummary(model, input_size=input_size, device=device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of evaluation metrics and train and Validation Dataloader for training purpose","metadata":{}},{"cell_type":"code","source":"\n# Dice score implementation\ndef dice_score(pred, target, smooth=1e-6):\n    pred = pred > 0.5  # Threshold predictions\n    target = target > 0.5\n    intersection = (pred * target).sum().float()\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    return dice\n\n\ndef train(dataloader, model, loss_fn, optimizer, lr_scheduler):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.train()\n    epoch_loss = 0\n    epoch_iou_score = 0\n    epoch_dice_score = 0\n\n    for batch_i, batch in enumerate(dataloader):\n        x, y = batch['x'].to(device), batch['y'].to(device)\n\n        optimizer.zero_grad()\n        pred = model(x)\n        #print(\"pred=\",pred)\n        loss = loss_fn(pred, y)\n\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n        pred = torch.sigmoid(pred)\n        #print(\"pred_sigmoid\",pred)\n        pred = pred.squeeze(dim=1)\n        \n        y = y.round().long()\n\n        # Calculate Dice score\n        dice = dice_score(pred, y)\n        epoch_dice_score += dice.item()\n\n        # Calculate IoU score\n        tp, fp, fn, tn = smp.metrics.get_stats(pred, y, mode='binary', threshold=0.5)\n        iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n        epoch_iou_score += iou\n\n        lr_scheduler.step()\n\n    return epoch_loss / num_batches, epoch_dice_score / num_batches, epoch_iou_score / num_batches\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    epoch_loss = 0\n    epoch_iou_score = 0\n    epoch_dice_score = 0\n\n    with torch.no_grad():\n        for batch_i, batch in enumerate(dataloader):\n            x, y = batch['x'].to(device), batch['y'].to(device)\n\n            pred = model(x)\n            loss = loss_fn(pred, y)\n\n            epoch_loss += loss.item()\n\n            pred = torch.sigmoid(pred)\n            pred = pred.squeeze(dim=1)\n            y = y.round().long()\n\n            # Calculate Dice score\n            dice = dice_score(pred, y)\n            epoch_dice_score += dice.item()\n\n            # Calculate IoU score\n            tp, fp, fn, tn = smp.metrics.get_stats(pred, y, mode='binary', threshold=0.5)\n            iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n            epoch_iou_score += iou\n\n    return epoch_loss / num_batches, epoch_dice_score / num_batches, epoch_iou_score / num_batches\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Definition of loss function","metadata":{}},{"cell_type":"code","source":"\ndef loss_fn(y_Pred, y_True, smooth=1e-6):\n    #print(\"Y_Pred, Y_True\", y_Pred.shape, y_True.shape)\n    loss_fn1 = smp.losses.DiceLoss(mode=\"binary\")\n    loss_dice = loss_fn1(y_Pred, y_True)\n    \n    return loss_dice\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Training","metadata":{}},{"cell_type":"code","source":"\nEPOCHS = 100\nlogs = {\n    'train_loss': [], 'val_loss': [],\n    'train_iou_score': [], 'val_iou_score': [],\n    'train_dice_score': [], 'val_dice_score': []\n}\n\n\nif os.path.exists('checkpoints') == False:\n    os.mkdir(\"checkpoints\")\n\n#loss_fn = smp.losses.DiceLoss(mode=\"binary\")\n#loss_fn = smp.losses.FocalLoss(mode=\"binary\")\n\n#loss_fn = DiceLoss(mode=\"binary\")\n#loss_fn = FocalLoss(mode=\"binary\")\n\nlearning_rate = 0.001\noptimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 100, gamma=0.1)\n\n# Earlystopping\npatience = 5\ncounter = 0\n#best_loss = np.inf\nbest_dice_score=-1\n\nmodel.to(device)\nfor epoch in tqdm(range(EPOCHS)):\n    train_loss, train_dice_score, train_iou_score = train(train_loader, model, loss_fn, optimizer, step_lr_scheduler)\n    val_loss, val_dice_score, val_iou_score = test(val_loader, model, loss_fn)\n\n    logs['train_loss'].append(train_loss)\n    logs['val_loss'].append(val_loss)\n    logs['train_dice_score'].append(train_dice_score)\n    logs['val_dice_score'].append(val_dice_score)\n    logs['train_iou_score'].append(train_iou_score)\n    logs['val_iou_score'].append(val_iou_score)\n\n    print(f'EPOCH: {str(epoch+1).zfill(3)} | '\n          f'train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f} | '\n          f'train_dice_score: {train_dice_score:.3f}, val_dice_score: {val_dice_score:.3f} | '\n          f'train_iou_score: {train_iou_score:.3f}, val_iou_score: {val_iou_score:.3f} | '\n          f'lr: {optimizer.param_groups[0][\"lr\"]}')\n\n    # Save model\n    torch.save(model.state_dict(), \"checkpoints/last.pth\")\n\n    if val_dice_score > best_dice_score:\n        counter = 0\n        best_dice_score = val_dice_score\n        torch.save(model.state_dict(), \"checkpoints/best.pth\")\n    else:\n        counter += 1\n\n    # Early stopping\n    if counter >= patience:\n        print(\"Early stopping!\")\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Displaying the loss curve and Dice Score curve","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(logs['train_loss'],label='Train_Loss')\nplt.plot(logs['val_loss'],label='Validation_Loss')\nplt.title('Train_Loss & Validation_Loss',fontsize=20)\nplt.legend()\n\n#plt.subplot(1,2,2)\n#plt.plot(logs['train_iou_score'],label='Train_Iou_Score')\n#plt.plot(logs['val_iou_score'],label='Validation_Iou_Score')\n#plt.title('Train_Iou_score & Validation_Iou_score',fontsize=20)\n#plt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(logs['train_dice_score'],label='Train_Dice_Score')\nplt.plot(logs['val_dice_score'],label='Validation_Dice_Score')\nplt.title('Train_Dice_score & Validation_Dice_score',fontsize=20)\nplt.legend()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_transforms = A.Compose([\n    A.Resize(512, 512),\n    # ToTensorV2(),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe,transforms_=None):\n        self.df = dataframe\n        self.transforms_ = transforms_\n        self.pre_normalize = v2.Compose([\n            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        self.resize = [512, 512]\n        self.class_size = 2\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        img = cv2.cvtColor(cv2.imread(self.df.iloc[index]['images']), cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.df.iloc[index]['masks'],cv2.IMREAD_GRAYSCALE)\n        aug = self.transforms_(image=img, mask=mask)\n        img, mask = aug['image'], aug['mask']\n        img_view = np.copy(img)\n        img = img/255\n        # img = self.pre_normalize(img)\n        img = torch.tensor(img, dtype=torch.float).permute(2, 0, 1)\n        mask_view = np.copy(mask)\n        mask = np.where(mask<127, 0, 1).astype(np.int16)\n        target = torch.tensor(mask, dtype=torch.long)\n        sample = {'x': img, 'y': target, 'img_view':img_view, 'mask_view':mask_view}\n        return sample\n\ntest_dataset = TestDataset(test_df, test_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=4)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Evaluation of our proposed model on the Test Dataset","metadata":{}},{"cell_type":"code","source":"\nmodel.load_state_dict(torch.load('checkpoints/best.pth'))\nmodel.to(device)\n\ndef get_metrics(model, dataloader, threshold):\n    IoU_score, precision, f1_score, recall, acc, dice_score = 0, 0, 0, 0, 0, 0\n    batches = 0\n    model.eval()\n    with torch.no_grad():\n        for batch_i, batch in enumerate(dataloader):\n            x, y = batch['x'].to(device), batch['y'].to(device)  # move data to GPU\n            pred = model(x)\n            pred = pred.squeeze(dim=1)\n            pred = torch.sigmoid(pred)\n            y = y.round().long()\n\n            # Calculate stats\n            tp, fp, fn, tn = smp.metrics.get_stats(pred, y, mode='binary', threshold=threshold)\n\n            # Calculate various metrics\n            batch_iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").item()\n            batch_acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\").item()\n            batch_f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item()\n            batch_recall = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro\").item()\n            batch_precision = smp.metrics.precision(tp, fp, fn, tn, reduction=\"micro\").item()\n\n            # Manually calculate Dice score\n            dice = (2 * tp.sum()) / (2 * tp.sum() + fp.sum() + fn.sum())\n            batch_dice_score = dice.item()\n\n            # Aggregate the results\n            IoU_score += batch_iou_score\n            acc += batch_acc\n            f1_score += batch_f1_score\n            recall += batch_recall\n            precision += batch_precision\n            dice_score += batch_dice_score\n            batches += 1\n\n    # Compute average metrics over all batches\n    IoU_score = round(IoU_score / batches, 3)\n    precision = round(precision / batches, 3)\n    f1_score = round(f1_score / batches, 3)\n    recall = round(recall / batches, 3)\n    acc = round(acc / batches, 3)\n    dice_score = round(dice_score / batches, 3)\n\n    sample = {\n        'iou': IoU_score,\n        'pre': precision,\n        'fi': f1_score,\n        're': recall,# Example Decoder class where Fused Attention is used in decoding stages\n        'acc': acc,\n        'dice': dice_score\n    }\n    return sample\n\n# Evaluate the model for different thresholds\nthreshold_list = [0.3, 0.4, 0.5, 0.6, 0.7]\nfor threshold in threshold_list:\n    sample = get_metrics(model, test_loader, threshold)\n    print(f\"Threshold: {threshold:.2f} \\\n    IoU Score: {sample['iou']:.3f} \\\n    Precision: {sample['pre']:.3f} \\\n    F1 Score: {sample['fi']:.3f} \\\n    Recall: {sample['re']:.3f} \\\n    Accuracy: {sample['acc']:.3f} \\\n    Dice Score: {sample['dice']:.3f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Displaying some input images and their corresponding predicted masks","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"checkpoints/best.pth\"))\nmodel.to(device)\n\nshow_imgs = 8\n\nrandom_list = np.random.choice(len(test_dataset), show_imgs, replace=False)\n\nfor i in range(show_imgs):\n    idx = random_list[i]\n    sample = test_dataset[idx]\n    pred = model(sample['x'].to('cuda', dtype=torch.float32).unsqueeze(0))\n    pred = torch.sigmoid(pred).squeeze(0).squeeze(0)\n    pred = pred.data.cpu().numpy()\n    pred = np.where(pred<0.5, 0, 1).astype(np.int16)\n    pred_img = Image.fromarray(np.uint8(pred), 'L')\n\n    img_view = sample['img_view']\n    img_view = Image.fromarray(img_view, 'RGB')\n\n    mask_view = sample['mask_view']\n    mask_view = Image.fromarray(mask_view, 'L')\n\n    f, axarr = plt.subplots(1, 3)\n    axarr[0].imshow(img_view)\n    axarr[0].set_title('Input')\n    axarr[0].axis('off')\n    axarr[1].imshow(pred_img)\n    axarr[1].set_title('pred')\n    axarr[1].axis('off')\n    axarr[2].imshow(mask_view)\n    axarr[2].set_title('gt')\n    axarr[2].axis('off')\n    plt.show()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Visualization for input image and its predicted mask and ground truth mask","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\n# Load the model and set it to evaluation mode\nmodel.load_state_dict(torch.load(\"checkpoints/best.pth\"))\nmodel.to(device)\nmodel.eval()\n\n# Define the path to the test image and ground truth mask\nimage_path = \"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Test_Data/ISIC_0010073.jpg\"\nmask_path = \"/kaggle/input/isic-2016-dataset/ISIC_2016_dataset/ISIC_2016_dataset/ISBI2016_ISIC_Part1_Test_GroundTruth/ISIC_0010073_Segmentation.png\"\n\n# Define preprocessing pipeline\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),  # Ensure input size matches model's requirements\n    transforms.ToTensor(),         # Convert PIL Image to PyTorch tensor\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n])\n\n# Preprocess the test image\nimage_test = Image.open(image_path).convert(\"RGB\")\nimage_tensor = transform(image_test).unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n\n# Predict the segmentation mask\nwith torch.no_grad():\n    pred = model(image_tensor)  # Pass the image through the model\n    pred = torch.sigmoid(pred).squeeze(0).squeeze(0).cpu().numpy()  # Apply sigmoid, remove batch dimension, and move to CPU\n    pred_binary = np.where(pred < 0.5, 0, 1).astype(np.uint8)  # Binarize the prediction\n\n# Load and preprocess the ground truth mask\nmask_view = Image.open(mask_path).resize((512, 512))  # Resize to match predicted mask size\nmask_view = np.array(mask_view)  # Convert to numpy array\n\n# Convert prediction to image format for visualization\npred_img = Image.fromarray(pred_binary * 255, 'L')  # Scale binary mask to [0, 255] for display\n\n# Visualization\nf, axarr = plt.subplots(1, 3, figsize=(15, 5))\naxarr[0].imshow(image_test)\naxarr[0].set_title('Input Image')\naxarr[0].axis('off')\n\naxarr[1].imshow(pred_img, cmap='gray')\naxarr[1].set_title('Predicted Mask')\naxarr[1].axis('off')\n\naxarr[2].imshow(mask_view, cmap='gray')\naxarr[2].set_title('Ground Truth Mask')\naxarr[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking for heatmap","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Step 1: Load the model (ensure it's defined as per your architecture)\n#model = UnetDecoder(encoder_channels=[64, 128, 256, 512, 1024], decoder_channels=[512, 256, 128, 64, 32])\nmodel.load_state_dict(torch.load('checkpoints/best.pth'))\nmodel.eval()\n\n# Step 2: Load and preprocess the input image\ndef load_image(image_path):\n    # Load the image\n    image = Image.open(image_path).convert(\"RGB\")  # Ensure it's RGB\n\n    # Define the transformations\n    preprocess = transforms.Compose([\n        transforms.Resize((512, 512)),  # Resize to match model input\n        transforms.ToTensor(),  # Convert to tensor\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n    ])\n\n    # Apply transformations\n    image_tensor = preprocess(image)\n    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n    return image_tensor\n\n# Load your input image (replace with your actual image path)\ninput_image_path = '/kaggle/input/monuseg/MonuSeg/Test/Images/TCGA-GL-6846-01A-01-BS1.png'  # Specify the path to your image                                   \ninput_tensor = load_image(input_image_path)\ninput_tensor = input_tensor.to('cuda')\n\n\n# Step 3: Set up the hook and perform Grad-CAM\ndef get_activation(layer):\n    def hook(model, input, output):\n        activation.append(output)\n    return hook\n\n# Choose the target layer for Grad-CAM\ntarget_layer = model.decoder.blocks[4].conv2[2]  # Modify this to your architecture\n\n# Register the hook\nactivation = []\nhook = target_layer.register_forward_hook(get_activation(target_layer))\n\n# Forward pass through the model\nwith torch.no_grad():\n    output = model(input_tensor)\n\n# Unregister the hook\nhook.remove()\n# Now you can access the activation\nactivation_map = activation[0]  # Get the activation from the hook\n\n# Assuming the output shape is [N, C, H, W]\n# For visualization, we need to take the average across the channels\nactivation_map = activation_map.mean(dim=1, keepdim=True)  # Shape: [N, 1, H, W]\nactivation_map = F.relu(activation_map)  # ReLU activation\n\n# Normalize the activation map for visualization\nactivation_map = (activation_map - activation_map.min()) / (activation_map.max() - activation_map.min())\nactivation_map = activation_map.squeeze().cpu().numpy()  # Convert to numpy for plotting\n\n# Plot the activation map\nplt.imshow(activation_map, cmap='inferno')  # or any other colormap\nplt.axis('off')  # Turn off axis\n#plt.colorbar()  # Show colorbar for reference\n#plt.title('Grad-CAM Activation Map')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}